\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{DPN Learner:Unsupervised Learning of Depth, Ego-Motion and Surface Normal from Video}

\author{Aman Raj\\
A53247556\\
{\tt\small amraj@eng.ucsd.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Menghe Zhang\\
A53208258\\
{\tt\small mez071@eng.ucsd.edu}
\and
Sarah Wang\\
A53276846\\
{\tt\small XXXX@eng.ucsd.edu}
\and
Tingwei Yu\\
A53281022\\
{\tt\small t3yu@eng.ucsd.edu}}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}

Learning to reconstruct depths and camera pose from a single image in unsupervised way via deep convolutional network is attracting significant attention since Zhou proposed the unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences\cite{zhou2017unsupervised}. Our work extends their works by estimating depth map, camera pose and surface normal map together in an end-to-end framework. We verified that deep network approaches benefit from explicit geometric cues and constraints. Therefore, we take advantage both instance-level and semantic-level segmentation to make the framework edge-aware, while use the consistency between depth and normal, yielding more robust prediction for both depths, surface normals and ego-motions. Evaluations are conducted on both outdoor(KITTI) and indoor(NYUv2) datasets, also we did ablation study for different geometric constraint scenarios.  

\end{abstract}

%------------------------------------------------------------------------

\section{Introduction}
Inferring camera poses, depths and surface normal of a scene at a detailed level is crucial for 3D scene understanding. Recovering these information from monocular images can be widely applied in various real-world applications. For instance, in robot navigation, this enables the robot to avoid obstacles and travel through unseen areas. The ability to estimate depth and surface normal also facilitates 3D reconstruction of the environment and can be applied in the field of augmented reality.
Recently, there has been remarkable achievements in unsupervised pose and depth estimation using monocular cameras. Works such as \cite{zhou2017unsupervised} has significantly reduced human effort to obtain large quantities of color-depth image pairs for training while also yielding comparable results to supervised approaches. The key concept is to use two consecutive image frames where one is used as the target frame and the other used as the source frame. The source frame is then warped to the target frame using the estimated pose and depth. Following works attempt to exploit more geometric cues and constraints to improve the depth results.\cite{yang2018lego} refines the estimated depth by leveraging the constraint between surface normal and depth map as well as using image gradients to represent edges where depth discontinuities can occur.  \cite{casser2018depth} takes additional instance-level segmentation information as input and predicts poses for the background and each object separately. \\
In our work, we study the effects of posing various geometric constraints on the quality of depth estimation. First, we show that by utilizing additional semantic segmentation information, the model is able to learn the boundaries of the scene objects and generate sharper depth results. Second, we investigate how edge detection lead to better depth predictions. Last, we compare the results of the normal-depth constraint used in \cite{yang2018lego} and the patch-based photometric consistency constraint used in  \cite{furukawa2010accurate}. Evaluation on the KITTI dataset demonstrates the effectiveness of different approaches.


%------------------------------------------------------------------------
\section{Related work}
\textbf{Warping-based view synthesis}
View synthesis aims to create new view of a specific subject starting from a number of pictures taken from other point of views. Classic paradigm for view synthesis to first either estimate the underlying 3D geometry explicitly or establish pixel correspondence among input views, and then synthesized the novel views by compositing image patches from the input views\cite{malik1996modeling}\cite{fitzgibbon2005image}\cite{DBLP:journals/tog/ZitnickKUWS04}. Another common approach is to synthesize images without explicitly estimate the geometry.For example, Mahajan\cite{shechtman2010regenerative} propose to move the gradients in the input images along a specific path to reconstruct the image at a novel view. Shechtman\cite{shechtman2010regenerative} propose a patch-based optimization framework to reconstruct images at novel views. Also, the end-to-end learning based framework DeepStereo\cite{DBLP:journals/corr/FlynnNPS15}, However, unlike the warping-based methods, such frameworks only work on the inputs and output, without learning intermediate predictions of geometry and correspondence. 
\\
\textbf{Unsupervised learning from videos} Unsupervised learning of visual representations has a
rich history starting from original auto-encoders work of Olhausen and Field\cite{olshausen1997sparse}. To get the visual representation from video, the general goal is to design pretext tasks for learning generic visual features, that are sparse and reconstructive, from video data that can later be re-purposed for other vision tasks. Researchers have started focusing on learning feature representations using videos. Early work such as\cite{zou2012deep} focused on inclusion of constraints via video to autoencoder framework. The most common constraint is enforcing learned representations to be temporally smooth. Similar to this, Goroshin et al.\cite{goroshin2015unsupervised} proposed to learn autoencoders based on the slowness prior. Other approaches such as Taylor et al.\cite{taylor2010convolutional} trained convolutional gated RBMs to learn latent representations from pairs of successive images. This was extended in a recent work by Srivastava et al.\cite{srivastava2015unsupervised} where they proposed to learn a LSTM model in an unsupervised manner to predict future frames.
\\
\textbf{Image segmentation}
Semantic segmentation with the goal to assign semantic labels to every pixel in an image is one of the fundamental topics in computer vision. Deep convolutional neural networks based on the Fully Convolutional Neural Network show striking improvement over systems relying on hand-crafted features on benchmark tasks. DeepLab is a state-of-art deep learning model for semantic image segmentation. The most state-of-the-art work, DeepLabv3+\cite{deeplabv3plus2018} extends DeepLabv3\cite{chen2017rethinking} to include a simple yet effective decoder module and a spatial pyramid pooling module to refine the segmentation results especially along object boundaries. Furthermore, in this encoder-decoder structure one can arbitrarily control the resolution of extracted encoder features by atrous convolution to trade-off precision and runtime.
%%%%%%%%%%%%%%%%5instance level here
\subsection{Supervised surface normal prediction}


%------------------------------------------------------------------------
\section{Method}
The approaches we stress here are used for jointly estimating depths, surface normal and ego-motions. The core underlying idea is inverse warping from target view to source view with awareness of 3D geometry, and a depth-normal consistency. 
\subsection{Network architecture}
\textbf{Depth and Normal Net}
For the depth as well as surface normal estimation, we adopted the DispNet(add related) architecture which uses an encoder followed by a decoder with skip connections and multi-scale side outputs. The depth and normal net share the same encoder while having different decoders to predict depth and normals separately. Other than the final output layer which has a sigmoid function applied to enforce the predictions in an reasonable range, the other conv layers of both nets are followed by ReLU activations.\\  
\textbf{Pose Net}
We adopted the pose net architecture purposed in (related work) for camera pose estimation. The input of the network is the target-source frame pairs, and the output is the 6D camera pose from each target frame to the source frame. All conv layers except for the final output layer, where no non-linear activation is applied, are followed by ReLU activations.
\subsection{View synthesis as supervision}
The baseline for our work is from Zhou et al.\cite{zhou2017unsupervised}. From the multiple view geometry, for a target view  image $I_t$ and a source view $I_s$, given an estimated depth map $D_t$ of target view and the estimated transformation with 6 DOF $T_{t\xrightarrow{}s}$ from $I_t$ to $I_s$, thus, for any pixel $p_t$ in target view $I_t$ can be found through perspective projection. Given such relationship, a syntehsized target view $\hat{I_s}$ can be generated from $I_s$ through bilinear interpolation. Finally, by comparing the photometric error between the original target view $I_t$ and the synthesized one $\hat{I_s}$, thus, problem now went to a supervised one. The photometric loss can be formulated as 
\begin{equation}
    L_{vs} = \sum_s\sum_p |I_t(p) - \hat{I_s}(p)|
\end{equation}
where $p$ indexes over pixel coordinates and $I_s$ is the source view $I_s$ warped to the target coordinate frame based on a depth image-based rendering module, taking the predicated depth $\hat{D_t}$, the predicted $4 \times 4$ camera transformation matrix $\hat{T_{t\xrightarrow{}s}}$ and the source view $I_s$ as input. Thus, the projected point
\begin{multline}
        \hat{I_s}(p_t) = I_s(p_s)=\sum_{i\in{t,b}, j\in{l,r}} w^{ij}I_s(p_s^{ij})
    \\
    where\quad p_s \sim K\hat{T_{t\xrightarrow{}s}}\hat{D_t}(p_t)K^{-1}p_t
\end{multline}
\textbf{Regularization of depth} Due to one pixel can match to many candidates. Thus, extra regularization is required to learn reasonable depth prediction. One common strategy is to encourage the estimated depth to be locally similar when no significant image gradient exists, that is 
\begin{equation}
    L_s(D_t, 2) = \sum_{pt}\sum_{d\in x, y}||\bigtriangledown_d^2D_t(p_t)||e^{-\alpha|\bigtriangledown_dI(p_t)|}
\end{equation}
\textbf{Photometric pixel loss} In addition to the reconstruction loss and smooth loss, Structural Similarity(SSIM)\cite{wang2004image} loss was applied to be part of pixel loss, where
\begin{equation}
    L_{pixel} = \alpha_{pixel} \frac{(2\mu_x\mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C1)(\sigma_x^2 + \sigma_y^2 + C_2)}
\end{equation}
\subsection{Edge-aware depth estimation}
There are two components to parameterize and then learn the geometrical edges. First is to use the image gradient: we compute a gradient map of the target image and synthesized target images, and make it a gradient matching error to the total loss function\cite{casser2018depth}.
\begin{equation}
    L_g(D_n, T, M) = \sum_{s=1}^S\sum_{x_t}M_s(x_t)||\bigtriangledown I_t(x_t) - \bigtriangledown\hat{I_s}(x_t)||_1
\end{equation}
Another way is to jointly learn the edge map $E_t$ for the target image from semantic mask. Here we have the intervening contour cue fro measuring the affinity between two pixels. Specifically, we parameterize the prediction of $E_t$ using a decoder network same to dispnet, and add another edge loss term(computed from semantic masks) to loss function.
\subsection{Depth-normal consistency constraints}
\textbf{Depth and normal orthogonality constraint} To train the normal network, we applied the depth-normal orthogonality constraint purposed in (xx) to our network. This could enforce the predicted surface normal to be perpendicular to its tangent plane. Where for each pixel $p_i$ in the target view, we compute sum of the dot products between the estimated normal and the vectors pointing from the pixel to it's 8 neighboring pixels as shown in figure ?. The equation is written as,
\begin{multline}
    L_{orth}(\hat{D_t}, \hat{N_t}) = \sum_{i\in t}\sum_{j\in Nei(i)}||[ \phi(x_j) - \phi(x_i)] \hat{N_t}(x_i)||_1 
    \\
    where\phi(x) = D_t(x)K^{-1}x
\end{multline}

where $Nei(x_i)$ is the set of 8 neighboring pixels of $x_i$, and $\hat N(x_i)$ and $\hat D(x_i)$ are the estimated surface normal and depth of the corresponding pixel. We define $\phi(x_i)$ as the back-projected 3D point of pixel $x_i$ in the 3D space, thus $\phi(x_j) - \phi(x_i)$ is the vector pointing from the center pixel to the neighboring pixels in the world coordinates. \\
Since we don't have direct supervision for $N(x_i)$, training for the above equation would result into always predicting the values in $\hat{N}$ to zero. Thus we concluded an additional regularization term $L_n$ in the loss function, where we enforce the norm the predicted surface normal to be 1.
\\
\textbf{Patch-based depth and normal constraint} Through experiments we observed that only enforcing the depth and normal orthogonality correlation could not provide our network enough geometry cues to reconstruct the surface normals.(fig.?) Thus, we applied the patched-based depth and normal constraint to our network based on previous works (add related works). For each pixel in the target image, we reconstructed a $n\times n$ patch in the 3D space with the center calculated by back-projecting the pixels(fig.?). The orientation of the patch is given by the estimated surface normal and we assign one of its edges to be parallel to the x-axis of the target frame. Through re-projecting each patch back in to the target image $I_t$ as well as the source image $I_s$, we sample the intensities of the patch in both views through bilinear interpolation to acquire $I_{p_ti}$ and $I_{p_si}$. The loss term $L_{patch}$ is then the sum of L1 difference of the intensities between each pair of patches. (fig. ?)
\subsection{Geometric structure refinement}
% \subsection{Photometric error from view synthesis}
%------------------------------------------------------------------------
\section{Experiments}
\subsection{Datasets and metrics}
We conducted experiments on depth and normal estimation. The performances are evaluated on two popular datasets: KITTI for outdoor scene and NYUv2 for indoor scene.
For depth evaluation, we adopted those used in Zhou et al.\cite{zhou2017unsupervised}, details are shown on table\ref{table1:depth_eval}
\begin{figure*}[t!]
  \includegraphics[width=\linewidth]{network.png}
  \caption{Network architectures for different experiments}
  \label{fig:network}
\end{figure*}

\begin{table*}
\centering
\caption{Depth evaluation metrics}
\label{table1:depth_eval}
\begin{tabular}{l|l}
 Threshold: $\% of y_i, s.t. max(\frac{y_i}{y_i^*}, \frac{y_i^*}{y_i})=\delta < thr $&  RMSE(linear):$\sqrt{\frac{1}{|T|}\sum_{y\inT}||y_i - y_i^*||^2}$ \\
 Abs Relative difference: $\frac{1}{|T|}\sum_{y\in T||y-y^*||/y^*}$ &  RMSE(log):$\sqrt{\frac{1}{|T|}\sum_{y \in T}||log y_i - log y_i^*||^2}$ \\
 Squared Relative difference:$\frac{1}{|T|}\sum_{y\in T||y-y^*||^2/y^*}$ & 
\end{tabular}
\end{table*}

For normal evaluation, we
implement the evaluation metrics in\cite{fouhey2013data}: in addition to reporting the mean, median, and
RMSE on a per-pixel-basis, we report three pixel-accuracy
metrics, or percent-good-pixels (i.e., the fraction of pixels with cosine distance to ground-truth less than t) with $t = 11.25\degree, 22.5\degree, 30\degree$.

\subsection{Depth and normal experiments}
Like \cite{zhou2017unsupervised}, We implemented the system using TensorFlow framework. For all the experiments, we set $\lambda_s = 0:5=l$ (l is the downscaling factor for the corresponding scale) and $\lambda_e = 0:2$. During training, we used batch normalization for all the layers except for the output layers, and the Adam optimizer with $\beta_1 = 0:9$, $\beta_2 = 0:999$, learning rate of $0:0002$ with $\%95\%$ decay and mini-batch size of 4. The training typically converges after about 150K iterations. All the experiments are performed with image sequences captured with a monocular camera. We resize the images to $128 × 416$ during training.
For ssim pixel loss, weight is set to 0.3. For depth edge-awareness training, we set the gradient weight to 0.5, while use the semantic mask as input to edge net and the weight is set to 0.3.
For normal map, there are two ways that make the normal map contribute to the final result: first is to set the normal smooth loss and second is to do depth-normal constraints. Here we apply normal smooth loss term the same to depth, 0.5. During the training times, we also test various weight to add depth-normal constriants(infer normal from depth and back compute depth from normal).\\
Similar to Yang et al.\cite{yang2018lego}, we evaluate the normal map(either from direct-prediction or from depth inference) by comparing to the ground truth normal, which is generated by applying depth-to-normal layer on interpolated depth ground truth. The normal evaluation is performed
on KITTI dataset. The comparison of normal evaluations on KITTI split is presented in???. 

\subsection{Direct normal prediction experiments}
In this section we analyze our approaches on directly predicting the normals qualitatively and quantitatively. 
\\
\textbf{Evaluation on the depth and normal orthogonalily constraint}
To evaluate the depth and normal orthogonality constraint, we first jointly train the depth and normal net by only leveraging the $L_{vs}$ and $L_{orth}$ in our loss function. Yet, the results aren't very promising.(table.?) The network can not generate meaningful surface normal and also degraded the depth predictions. Thus, we independently train the normal net along with the pre-trained depth net to obtained more meaningful results. As can be seen in figure xx, we could roughly determine the road of the scene but geometric detail information are not observed by the network.
\\
\textbf{Evaluation on the patch-based photometric constraint}
We evaluate the patch-based photometric constraint by adding the $L_{orth}$ term in our loss function. In Fig.? we show a qualitative result of this approach. As can been seen, the estimated normal could roughly capture the contours of the scene; however, there are still some spurious blob in the image. 







\subsection{Results}






\begin{figure*}[t!]
  \includegraphics[width=\linewidth]{output.jpg}
  \caption{Experiment results on KITTI}
  \label{fig:KITTI}
\end{figure*}

\begin{figure*}[t!]
  \includegraphics[width=\linewidth]{patch_based.jpg}
  \caption{Patch-based experiment results on KITTI}
  \label{fig:KITTI}
\end{figure*}


%------------------------------------------------------------------------
\section{Conclusion and Future Work}
Based on the notion that deep network approaches benefit from explicit geometric cues and constraints, we tried out a bunch of criteria to get better depth, normal and ego-motion prediction by offering geometric cues and set the consistency constraints. Ablation study show the results we found that take effects and those do not. \\
Future work is to try refinement methodology as explained in \cite{casser2018depth} to further improve the depth prediction. Also, improve surface normals using our best depth prediction model, 
Patch-based loss, to simultaneously improve depth and normal.
Generalize model for depth and normal prediction to other datasets like Cityscape.


%------------------------------------------------------------------------

% \begin{table}
% \begin{center}
% \begin{tabular}{|l|c|}
% \hline
% Method & Frobnability \\
% \hline\hline
% Theirs & Frumpy \\
% Yours & Frobbly \\
% Ours & Makes one's heart Frob\\
% \hline
% \end{tabular}
% \end{center}
% \caption{Results.   Ours is better.}
% \end{table}

% {\small\begin{verbatim}
%   \usepackage[dvips]{graphicx} ...
%   \includegraphics[width=0.8\linewidth]
%                   {myfile.eps}
% \end{verbatim}
% }
% \begin{figure}[t]
% \begin{center}
% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
% \end{center}
%   \caption{Example of caption.  It is set in Roman so that mathematics
%   (always set in Roman: $B \sin A = A \sin B$) may be included without an
%   ugly clash.}
% \label{fig:long}
% \label{fig:onecol}
% \end{figure}



% \begin{figure*}
% \begin{center}
% \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
% \end{center}
%   \caption{Example of a short caption, which should be centered.}
% \label{fig:short}
% \end{figure*}



{\small
\bibliographystyle{ieee}
\bibliography{refs}
}

\end{document}
